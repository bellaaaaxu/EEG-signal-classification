# -*- coding: utf-8 -*-
"""SVM+LFCNN+KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H4tAmUcWxJVl8ntpy9YalODarHITOpsP

# Install packages and Inner Speech Github code
"""

!git clone https://github.com/N-Nieto/Inner_Speech_Dataset.git
!pip install mne 
!pip install pickle
!pip install mneflow
!pip install pyriemann

from google.colab import drive
drive.mount('/content/drive')

"""# Load Data"""

import mne 
import pickle
import numpy as np

import sys
# insert at 1, 0 is the script path (or '' in REPL)
sys.path.insert(1, '/content/Inner_Speech_Dataset/Python_Processing')
sys.path.insert(1, '/content/drive/MyDrive/CPSC554X_Project/dataset')


from Events_analysis    import Event_correction, Add_condition_tag, Add_block_tag, Delete_trigger
from Events_analysis    import Cognitive_control_check, Standarized_labels, Check_Baseline_tags
from Data_extractions   import Extract_subject_from_BDF
from Utilitys           import Ensure_dir
from AdHoc_modification import adhoc_Subject_3
from EMG_Control        import EMG_control_single_th

# Root where the raw data are stored 
#root_dir = '../'
root_dir = '/content/drive/MyDrive/CPSC554X_Project/dataset/'

# Root where the structured data will be saved - It can be changed and saved in other direction
save_dir = root_dir + "derivatives/"

# Subjects and blacks 
#N_Subj_arr = [1,2,3,4,5,6,7,8,9,10]
N_Subj_arr = [1,2,3,4,5,6,7,8]
N_block_arr = [1,2,3]

##################### Filtering
# Cut-off frequencies
Low_cut = 0.5
High_cut = 100

# Notch filter in 50Hz
Notch_bool = True

# Downsampling rate
DS_rate = 4

##################### ICA 
# If False, ICA is not applyed
ICA_bool = True
ICA_Components = None 
ica_random_state = 23
ica_method = 'infomax'
max_pca_components = None
fit_params = dict(extended=True)

##################### EMG Control
low_f = 1
high_f = 20
# Slide window desing
# Window len (time in sec)
window_len = 0.5
# slide window step (time in sec)
window_step = 0.05

# Threshold 
std_times = 3

# Baseline
t_min_baseline = 0
t_max_baseline = 15

# Trial time
t_min = 1
t_max = 3.5

# In[]: Fixed Variables
# Events ID 
# Trials tag for each class. 
# 31 = Arriba / Up 
# 32 = Abajo / Down
# 33 = Derecha / Right
# 34 = Izquierda / Left
event_id = dict(Arriba = 31, Abajo = 32, Derecha = 33, Izquierda = 34)

#Baseline id
baseline_id = dict(Baseline = 13)

# Report initialization
report = dict(Age = 0, Gender = 0, Recording_time = 0,  Ans_R = 0, Ans_W = 0)

# Montage
Adquisition_eq = "biosemi128"
# Get montage
montage = mne.channels.make_standard_montage(Adquisition_eq)

# Extern channels
Ref_channels = ['EXG1', 'EXG2']

# Gaze detection
Gaze_channels = ['EXG3','EXG4']

# Blinks detection
Blinks_channels = ['EXG5','EXG6']

# Mouth Moving detection
Mouth_channels = ['EXG7','EXG8'] 

# Demographic information
Subject_age = [56,50,34,24,31,29,26,28,35,31]

Subject_gender = ['F','M','M','F','F','M','M','F','M','M'] 

# In[] = Processing loop

"""### (don't need to run this cell again since it's already in the drive) """

for N_S in N_Subj_arr:
    # Get Age and Gender
    report['Age'] = Subject_age[N_S-1]
    report['Gender'] = Subject_gender[N_S-1]
    
    for N_B in N_block_arr:
        print('Subject: ' + str (N_S))
        print('Session: ' + str (N_B))
        
        # Load data from BDF file
        rawdata, Num_s = Extract_subject_from_BDF(root_dir,N_S,N_B)
        
        # Referencing
        rawdata.set_eeg_reference(ref_channels=Ref_channels) 
        
        if Notch_bool:
            # Notch filter
            rawdata = mne.io.Raw.notch_filter(rawdata,freqs=50)

        # Filtering raw data
        rawdata.filter(Low_cut, High_cut)
        
        # Get events
        # Subject 10  on Block 1 have a spureos trigger
        if (N_S == 10 and N_B==1):
            events = mne.find_events(rawdata, initial_event = True, consecutive = True,min_duration = 0.002)  
            # The different load of the events delet the spureos trigger but also the Baseline finish mark
        else:
            events = mne.find_events(rawdata, initial_event=True, consecutive=True)    
            
        events = Check_Baseline_tags(events)
            
        # Check and Correct event
        events = Event_correction (N_S=N_S,N_E=N_B,events=events)
        
        # replace the raw events with the new corrected events
        rawdata.event = events
        
        report['Recording_time'] = int(np.round(rawdata.last_samp/rawdata.info['sfreq']))
        
        # Cognitive Control 
        report['Ans_R'] , report['Ans_W'] = Cognitive_control_check(events)

        # In[] Save report
        file_path = save_dir + Num_s + '/ses-0'+ str(N_B) 
        Ensure_dir(file_path)
        file_name = file_path + '/' +Num_s+'_ses-0'+str(N_B)+'_report.pkl'
        with open(file_name, 'wb') as output:
            pickle.dump(report, output, pickle.HIGHEST_PROTOCOL)
            
        # In[]:EXG
        #  the EXG Channels for saving
        picks_eog = mne.pick_types(rawdata.info, eeg = False, stim = False, include = ['EXG1', 'EXG2', 'EXG3', 'EXG4', 'EXG5', 'EXG6', 'EXG7', 'EXG8'])
        epochsEOG = mne.Epochs(rawdata, events, event_id = event_id, tmin = -0.5, tmax = 4,
                               picks = picks_eog, preload = True, detrend = 0, decim = DS_rate)
        
        # Save EOG
        file_name = file_path + '/' +Num_s + '_ses-0' + str(N_B) + '_exg-epo.fif'
        epochsEOG.save(file_name, fmt='double', split_size='2GB', overwrite=True)
        del epochsEOG
        
        # In[]: Baseline
        # Extract Baseline        
        # Calculate the Baseline time 
        t_baseline = (events[events[:,2]==14,0]-events[events[:,2]==13,0])/rawdata.info['sfreq']
        t_baseline = t_baseline[0]        
        Baseline = mne.Epochs(rawdata, events, event_id = baseline_id, tmin = 0, tmax = round(t_baseline),
                              picks = 'all', preload = True, detrend = 0, decim = DS_rate, baseline = None)
        
        # Save Baseline
        file_name = file_path + '/' +Num_s + '_ses-0' + str(N_B) +'_baseline-epo.fif'
        Baseline.save(file_name, fmt = 'double', split_size = '2GB', overwrite = True)
        del Baseline      
        
        # In[ ] Epoching and decimating EEG
        picks_eeg = mne.pick_types(rawdata.info, eeg=True,exclude=['EXG1', 'EXG2', 'EXG3', 'EXG4', 'EXG5', 'EXG6', 'EXG7', 'EXG8'], stim = False)
        epochsEEG = mne.Epochs(rawdata, events, event_id = event_id, tmin = -0.5, tmax = 4,
                               picks = picks_eeg, preload = True, detrend = 0, decim = DS_rate, baseline = None)
        
        # In[]: ICA Prosessing
        
        if ICA_bool:
            # Get a full trials including EXG channels
            picks_vir = mne.pick_types(rawdata.info, eeg=True, include=['EXG1', 'EXG2', 'EXG3', 'EXG4', 'EXG5', 'EXG6', 'EXG7', 'EXG8'], stim=False)
            epochsEEG_full=mne.Epochs(rawdata, events, event_id=event_id, tmin=-0.5, tmax=4,
                                      picks=picks_vir, preload=True, detrend=0, decim=DS_rate, baseline = None)
            
            # Liberate Memory for ICA processing
            del rawdata 
            
            # Creating the ICA object
            ica=mne.preprocessing.ICA(n_components=ICA_Components,random_state=ica_random_state, method=ica_method,fit_params=fit_params)
            
            # Fit ICA, calculate components
            ica.fit(epochsEEG)
            ica.exclude = []    
            
            # Detect sources by correlation
            exg_inds_EXG3, scores_ica = ica.find_bads_eog(epochsEEG_full, ch_name='EXG3')  # find via correlation
            ica.exclude.extend(exg_inds_EXG3)

            # Detect sources by correlation
            exg_inds_EXG4, scores_ica = ica.find_bads_eog(epochsEEG_full, ch_name='EXG4')  # find via correlation  
            ica.exclude.extend(exg_inds_EXG4)
            
            # Detect sources by correlation
            exg_inds_EXG5, scores_ica = ica.find_bads_eog(epochsEEG_full, ch_name='EXG5')  # find via correlation
            ica.exclude.extend(exg_inds_EXG5)
            
            # Detect sources by correlation
            exg_inds_EXG6, scores_ica = ica.find_bads_eog(epochsEEG_full, ch_name='EXG6')  # find via correlation
            ica.exclude.extend(exg_inds_EXG6)
            
            # Detect sources by correlation
            exg_inds_EXG7, scores_ica = ica.find_bads_eog(epochsEEG_full, ch_name='EXG7')  # find via correlation
            ica.exclude.extend(exg_inds_EXG7)
            
            # Detect sources by correlation
            exg_inds_EXG8, scores_ica = ica.find_bads_eog(epochsEEG_full, ch_name='EXG8')  # find via correlation
            ica.exclude.extend(exg_inds_EXG8)
            
            print("Appling ICA")
            ica.apply(epochsEEG)
        
        # In[]
        # Save EEG
        file_name = file_path + '/' +Num_s + '_ses-0' + str(N_B) + '_eeg-epo.fif'
        epochsEEG.save(file_name, fmt='double', split_size='2GB', overwrite=True)
         
        # In[]: Standarize and save events
        events = Add_condition_tag(events)
        events = Add_block_tag(events,N_B=N_B)
        events = Delete_trigger(events)
        events = Standarized_labels (events)

        # Save events
        file_name = file_path + '/' +Num_s + '_ses-0' + str(N_B) + '_events.dat'
        events.dump(file_name)

# In[]: Ad Hoc Modifications
adhoc_Subject_3(root_dir=root_dir)

# In[]: EMG Control
EMG_control_single_th(root_dir=root_dir,N_Subj_arr=N_Subj_arr,N_block_arr=N_block_arr,
                      low_f=low_f,high_f=high_f,t_min=t_min,t_max=t_max,window_len=window_len,
                      window_step=window_step,std_times=std_times,t_min_baseline=t_min_baseline,
                      t_max_baseline=t_max_baseline)

"""# Visualization"""



import matplotlib.pyplot as plt

from Data_extractions import Extract_block_data_from_subject, Extract_data_from_subject
from Data_processing import Filter_by_condition, Filter_by_class
from Utilitys import Ensure_dir, picks_from_channels

# In[] Imports modules

# Root where the data are stored
save_dir = root_dir+ "/ERPs/"

# Subjets
N_S_list=[1,2,3,4,5,6,7,8,9,10]
N_S_list=[1,2,3,4,5,6,7,8]
# Data Parameters
datatype="EEG"
Condition_list=["Pron"]
Classes_list=[ "All"]
channels = "all"

# Saving Parameters
save_bool = True
prefix="ERPs"

#Fix all random states
random_state = 23
np.random.seed(random_state)

#Plotting
# Time Windows
t_start = -0.5
t_end = 4
spatial_colors = True
plot_cues_bool = True
clim = dict(eeg=[-10, 15])
ylim = clim
bandwidth = 2
fontsize = 20
plt.rcParams.update({'font.size': fontsize})
plt.rcParams.update({'legend.framealpha':0})

# In[]: Load Data
N_B = 1
N_S = 1

# Load a single subject to use the Epoched Object structure
X_S , Y = Extract_block_data_from_subject(root_dir,N_S,datatype,N_B=N_B)

Adquisition_eq="biosemi128"
montage = mne.channels.make_standard_montage(Adquisition_eq)
X_S.set_montage(montage)

# Get picks for the selected channels
picks = picks_from_channels(channels)

n_test = 1
for Classes in Classes_list: 
    for Cond in Condition_list:
        count=1
        for N_S in N_S_list:

            # Load full subject's data
            X, Y = Extract_data_from_subject(root_dir, N_S, datatype)
        
            # Filter by condition
            X_cond , Y_cond = Filter_by_condition(X, Y, Condition = Cond)
        
            # Filter by class
            X_class , Y_class =  Filter_by_class(X_cond,Y_cond,Class=Classes)
            
            if count==1:  
                X_data = X_class
                Y_data = Y_class
                count=2
            else:
                X_data = np.vstack((X_data,X_class))
                Y_data = np.vstack((Y_data,Y_class))

        # In[]: Plotting     
        print("Ploting ERPs for Class: " + Classes +" in Condition: " + Cond )
        print("with the information of Subjects: " + str(N_S_list ))        
        
        # Put all data
        X_S._data=X_data
        X_S.events=Y_data
        X_averaged=X_S.average()

        fig= plt.figure(figsize=(20,10))
        axs= fig.add_axes([0.1,0.1,0.8,0.8])
        
        # Plot Cues
        if plot_cues_bool:
            plt.plot([0,0],[-15,15],axes=axs,color='black')
            plt.plot([0.5,0.5],[-15,15],axes=axs,color='black')
            plt.plot([3,3],[-15,15],axes=axs,color='black')
        
        # Plot ERPs
        X_averaged.plot(spatial_colors = spatial_colors, picks = picks, ylim = ylim, axes = axs, xlim= [t_start , t_end])

        title = "ERPs - Condition: " + Cond + " in Class" + Classes
        axs.set_title(title, fontsize = fontsize)
        
        # Save Figure
        if save_bool:
            Ensure_dir(save_dir)
            fig.savefig(save_dir + prefix + '_' + Cond + '_' + Classes + '_' + channels + '_.png', transparent = True)

# -*- coding: utf-8 -*-

"""
@author: Nicol√°s Nieto - nnieto
Power Spectral Density
"""
# In[] Imports modules

import mne
import numpy as np
import matplotlib.pyplot as plt

from Data_extractions import Extract_block_data_from_subject, Extract_data_from_subject
from Data_processing import Filter_by_condition, Filter_by_class
from Utilitys import Ensure_dir, picks_from_channels


# In[]: Processing Variables

# Root where the data are stored
#root_dir = "/media/nnieto/50e793fa-4717-4997-b590-b4167be63ee7/home/nnieto/Escritorio/Nico/Doctorado/Tesis/LIAA - sinc(i)/Toma de datos/Dataset InnerSpeech/"
save_dir = root_dir+"/PSD_Spectrum/"

N_S_list=[1,2,3,4,5,6,7,8]
#N_S_list=[1]
# Data Parameters
datatype="EEG"
Condition_list=["Pron","In", "Vis"]
Classes_list=[ "up", "down", "left", "right"]
#Classes_list=["All"]
channel = "A"
# Get picks for the selected channels
picks = picks_from_channels(channel)
# picks = ["A26"]
channels = ["CL", "CR", "OPZ", "OL", "OR"]

save_bool = True
prefix="Power_Spectral_density_"

#Fix all random states
random_state = 23
np.random.seed(random_state)

#Plotting
# Time Windows
tmin = 1
tmax = 3

fmin = 11
fmax = 32

y_min = 19
y_max = 31
bandwidth = 1
fontsize = 23


plt.rcParams.update({'font.size': fontsize})
plt.rcParams.update({'legend.framealpha':0})
colors = ["darkred","midnightblue","darkcyan", "darkgreen"]    # "midnightblue" - "darkred" - "darkcyan" - "darkgreen"

# In[]: Fix Parameters

N_B = 1
N_S = 1
# Load a single subject to use the Epoched Object structure
X_S,Y= Extract_block_data_from_subject(root_dir,N_S,datatype,N_B=N_B)

Adquisition_eq="biosemi128"
montage=mne.channels.make_standard_montage(Adquisition_eq)
X_S.set_montage(montage)



# In[]: Load Data
fig = plt.figure(figsize = [13, 10])
axs = plt.axes()
n_plot=0
for chan in channels:
  for c2, Classes in enumerate(Classes_list): 
    fig = plt.figure(figsize = [13, 10])
    axs = plt.axes()
    n_plot=0
    for c1, Cond in enumerate(Condition_list):
        count=1
        for N_S in N_S_list:

            # Load full subject's data
            X, Y = Extract_data_from_subject(root_dir, N_S, datatype)
            # Filter by condition
            X_cond , Y_cond = Filter_by_condition(X, Y, Condition = Cond)
        
            # Filter by class
            X_class , Y_class =  Filter_by_class(X_cond,Y_cond,Class=Classes)
            
            if count==1:  
                X_data = X_class
                Y_data = Y_class
                count  = 2
            else:
                X_data = np.vstack((X_data,X_class))
                Y_data = np.vstack((Y_data,Y_class))

        # In[]: Plotting            
        # Put all data
        X_S._data = X_data
        X_S.events = Y_data

        #color = colors[n_plot]
        # for next color
        n_plot = n_plot + 1
        picks = picks_from_channels(chan)
        color = colors[n_plot-1]
        #color = ((c2+1)/len(Classes_list)*(c1 == 0), (c2+1)/len(Classes_list)*(c1 == 1), (c2+1)/len(Classes_list)*(c1 == 2)) 
        fig = X_S.plot_psd(average = True , dB = True, estimate = "power", bandwidth = bandwidth,
                          color = color, picks = picks, fmin = fmin, 
                          fmax = fmax, tmin = tmin , tmax = tmax, ax = axs , )
    # In[]: Saving

    #legend_list = []

    #for i in Classes_list:
    #  for j in ["Pronounced ", "Inner Speech ", "Visualized "]:
    #    legend_list.append(j+i)

    axs.legend( ["Pronounced ", "Inner Speech ", "Visualized "], loc='upper right', borderaxespad=0.9,fontsize=fontsize,shadow=False) 
    axs.set_title(" ",fontsize=fontsize)
    axs.set_ylim(bottom= y_min, top=y_max)

    title = "Power Spectral Density"
    fig.suptitle(title)   

    # Save Figure
    if save_bool:
        Ensure_dir(save_dir)
        fig.savefig(save_dir + prefix + '_' +Classes+"_"+ chan + '_Nov13.png', transparent = True)

"""# SVM

### This part is for a single test
"""

# Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
#          Romain Trachel <romain.trachel@inria.fr>
#
# License: BSD (3-clause)

import numpy as np
import matplotlib.pyplot as plt

import mne
from mne import io

# from mne.datasets import sample
# Root where the raw data are stored 
#root_dir = '../'
root_dir = '/content/drive/MyDrive/CPSC554X_Project/dataset/'

N_S_list = [1,2,3,4,5,6,7,8]
N_B_arr=[1,2,3]
S = 0
epochs = []
data=dict()
y=dict()

# Data Parameters
datatype="EEG"
Condition_list=["Pron","In", "Vis"]
Classes_list=[ "up", "down", "left", "right"]

for N_S in N_S_list:
    for N_B in N_B_arr:

        # name correction if N_Subj is less than 10
        if N_S<10:
            Num_s='sub-0'+str(N_S)
        else:
            Num_s='sub-'+str(N_S)
            
        file_name = root_dir + '/derivatives/' + Num_s + '/ses-0'+ str(N_B) + '/' +Num_s+'_ses-0'+str(N_B)+'_eeg-epo.fif'
        X = mne.read_epochs(file_name,verbose='WARNING')
        epochs.append(X)
        data[N_B]= X._data

        file_name = root_dir + '/derivatives/' + Num_s + '/ses-0'+ str(N_B) + '/' +Num_s+'_ses-0'+str(N_B)+'_events.dat'
        y[N_B] = np.load(file_name,allow_pickle=True)



X._data

epochs



y

Y

[psd_all, freqs] = mne.time_frequency.psd_welch(X_S)

"""### Classify Paradigm: Event = {0,1,2} -> Pron-Inner-Vis"""

import numpy as np
import matplotlib.pyplot as plt

import mne
from mne import io
from Data_extractions import Extract_block_data_from_subject, Extract_data_from_subject
from Data_processing import Filter_by_condition, Filter_by_class
from Utilitys import Ensure_dir, picks_from_channels

root_dir = '/content/drive/MyDrive/CPSC554X_Project/dataset'
N_S_list = [1,2,3,4,5,6,7,8]

N_B_arr=[1,2,3]
S = 0
epochs = []
data=dict()
y=dict()

# Data Parameters
datatype="EEG"
#Condition_list=["Pron","In", "Vis"]
Condition_list = ["all"]
#Classes_list=[ "up", "down", "left", "right"]
Classes_list = [ "all"]
#channels = ["CL", "CR", "OPZ", "OL", "OR"] # paradigm
channels = ["OZ", "OL", "CR", "CL", "FZ", "FR"] #class
picks = []

# Get picks for the selected channels
for i in channels:
  picks.append(picks_from_channels(i))

picks = np.concatenate(picks)
picks.tolist()

# In[]: Load Data
N_B = 1
N_S = 1

# Load a single subject to use the Epoched Object structure
X_S , Y = Extract_block_data_from_subject(root_dir,N_S,datatype,N_B=N_B)

Adquisition_eq="biosemi128"
montage = mne.channels.make_standard_montage(Adquisition_eq)
X_S.set_montage(montage)

# Get picks for the selected channels
count=1

for Classes in Classes_list: 
  for Cond in Condition_list:

    for N_S in N_S_list:

        # Load full subject's data
        X1, Y1 = Extract_data_from_subject(root_dir, N_S, datatype)

        # Filter by condition
        X_cond , Y_cond = Filter_by_condition(X1,Y1,Condition=Cond)

        # Filter by class
        X_class , Y_class = Filter_by_class(X_cond,Y_cond,Class=Classes)
        
        if count==1:  
            X_data = X_class
            Y_data = Y_class
        #    X_data = X1
        #    Y_data = Y1
            count=2
        else:
            #X_data = np.vstack((X_data,X1))
            #Y_data = np.vstack((Y_data,Y1))
            X_data = np.vstack((X_data,X_class))
            Y_data = np.vstack((Y_data,Y_class))

# Put all data
X_S._data=X_data
X_S.events=Y_data
X_S = X_S.pick_channels(picks)
X_S = X_S.crop(tmin = 0, tmax = 4)
X_data = X_S._data
labels = X_S.events[:, 2]

print(labels)
print(X_S.events)

# plot CSP patterns estimated on full data for visualization
from mne.decoding import CSP  # noqa
n_components = 2  # pick some components(default=4)
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
csp.fit_transform(X_S.get_data(), labels)
data = csp.patterns_

import matplotlib.pyplot as plt

evoked = X_S.average() # https://mne.tools/stable/auto_tutorials/evoked/10_evoked_overview.html

fig, axes = plt.subplots(1, 2)
for idx in range(2):
    mne.viz.plot_topomap(data[idx], X_S.info, axes=axes[idx], show=False)
fig.suptitle('CSP patterns')
fig.tight_layout()
mne.viz.utils.plt_show()

# https://mne.tools/stable/auto_examples/decoding/decoding_csp_eeg.html
csp.plot_patterns(evoked.info, ch_type='eeg', units='Patterns (AU)', size=2)

print(evoked.info)

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)

#epochs_data = psd_all
epochs_data = X_data

scores = []
# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)

cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)

clf = Pipeline([('CSP', csp), ('SVC', svc)])

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))
    
# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.454842 / Chance level: 0.800000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)

#epochs_data = psd_all
#epochs_data = X_data
epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)

scores = []
# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)

cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)

clf = Pipeline([('CSP', csp), ('SVC', svc)])

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))
    
# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.438401 / Chance level: 0.800000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=1, kernel='sigmoid')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
epochs_data = X_data

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))


# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.359459 / Chance level: 0.800000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=1, kernel='sigmoid')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
#epochs_data = X_data
epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))


# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.369172 / Chance level: 0.800000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

n_components = 4  # pick some components(default=4)
svc = SVC(kernel='rbf', class_weight='balanced', C=10, gamma=1)
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
epochs_data = X_S.get_data()

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))

class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.597185 / Chance level: 0.800000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

n_components = 4  # pick some components(default=4)
svc = SVC(kernel='rbf', class_weight='balanced', C=10, gamma=1)
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
#epochs_data = psd_all
#epochs_data = X_data
epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))

class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.530377 / Chance level: 0.800000

"""### Classify Word: Event = {31,32,33,34} -> Up-Down-Right-Left"""

import numpy as np
import matplotlib.pyplot as plt

import mne
from mne import io
from Data_extractions import Extract_block_data_from_subject, Extract_data_from_subject
from Data_processing import Filter_by_condition, Filter_by_class
from Utilitys import Ensure_dir, picks_from_channels

root_dir = '/content/drive/MyDrive/CPSC554X_Project/dataset'
N_S_list = [1,2,3,4,5,6,7,8]
N_B_arr=[1,2,3]
S = 0
epochs = []
data=dict()
y=dict()

# Data Parameters
datatype="EEG"
#Condition_list=["Pron","In", "Vis"]
Condition_list = ["Inner"]
#Classes_list=[ "up", "down", "left", "right"]
Classes_list = [ "all"]
channels = ["CL", "CR", "OPZ", "OL", "OR"] # paradigm
#channels = ["OZ", "OL", "CR", "CL", "FZ", "FR"] #class
picks = []

# Get picks for the selected channels
for i in channels:
  picks.append(picks_from_channels(i))

picks = np.concatenate(picks)
picks.tolist()

# In[]: Load Data
N_B = 1
N_S = 1

# Load a single subject to use the Epoched Object structure
X_S , Y = Extract_block_data_from_subject(root_dir,N_S,datatype,N_B=N_B)

Adquisition_eq="biosemi128"
montage = mne.channels.make_standard_montage(Adquisition_eq)
X_S.set_montage(montage)

# Get picks for the selected channels
count=1

for Classes in Classes_list: 
  for Cond in Condition_list:

    for N_S in N_S_list:

        # Load full subject's data
        X1, Y1 = Extract_data_from_subject(root_dir, N_S, datatype)

        # Filter by condition
        X_cond , Y_cond = Filter_by_condition(X1,Y1,Condition=Cond)
        Y_cond[:,1] = Y_cond[:,1] + 31
        Y_cond[:, [1, 2]] = Y_cond[:, [2, 1]]
        
        # Filter by class
        X_class , Y_class = Filter_by_class(X_cond,Y_cond,Class=Classes)
        
        if count==1:  
            X_data = X_class
            Y_data = Y_class
        #    X_data = X1
        #    Y_data = Y1
            count=2
        else:
            #X_data = np.vstack((X_data,X1))
            #Y_data = np.vstack((Y_data,Y1))
            X_data = np.vstack((X_data,X_class))
            Y_data = np.vstack((Y_data,Y_class))

# Put all data
X_S._data=X_data
X_S.events=Y_data
X_S = X_S.pick_channels(picks)
X_S = X_S.crop(tmin = 0, tmax = 4)
X_data = X_S._data
labels = X_S.events[:, 2]

print(labels)
print(X_S.events)# plot CSP patterns estimated on full data for visualization

from mne.decoding import CSP  # noqa
n_components = 2  # pick some components(default=4)
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
csp.fit_transform(X_S.get_data(), labels)
data = csp.patterns_

import matplotlib.pyplot as plt

evoked = X_S.average() # https://mne.tools/stable/auto_tutorials/evoked/10_evoked_overview.html

fig, axes = plt.subplots(1, 2)
for idx in range(2):
    mne.viz.plot_topomap(data[idx], X_S.info, axes=axes[idx], show=False)
fig.suptitle('CSP patterns')
fig.tight_layout()

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)

#epochs_data = psd_all
epochs_data = X_S.get_data()

scores = []
# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)

cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)

clf = Pipeline([('CSP', csp), ('SVC', svc)])

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))
    
# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.208807 / Chance level: 0.750000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)

#epochs_data = psd_all
#epochs_data = X_data
epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)

scores = []
# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)

cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)

clf = Pipeline([('CSP', csp), ('SVC', svc)])

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))
    
# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.232883 / Chance level: 0.750000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

n_components = 4  # pick some components(default=4)
svc = SVC(kernel='rbf', class_weight='balanced', C=10, gamma=1)
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
epochs_data = X_S.get_data()

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))

class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.236080 / Chance level: 0.750000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

n_components = 4  # pick some components(default=4)
svc = SVC(kernel='rbf', class_weight='balanced', C=10, gamma=1)
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
#epochs_data = psd_all
#epochs_data = X_data
epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))

class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.257367 / Chance level: 0.750000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

n_components = 4  # pick some components(default=4)
svc = SVC(kernel='rbf', class_weight='balanced', C=10, gamma=1)
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
#epochs_data = psd_all
#epochs_data = X_data

[psd_all, freqs] = mne.time_frequency.psd_welch(X_S)
epochs_data = psd_all
#epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
#labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))

class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.262784 / Chance level: 0.750000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=1, kernel='sigmoid')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
epochs_data = X_data

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))


# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.243182 / Chance level: 0.750000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=1, kernel='sigmoid')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
#epochs_data = X_data
epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))


# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.253737 / Chance level: 0.750000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=10, kernel='poly')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
epochs_data = X_data

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))


# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.232102 / Chance level: 0.750000

from sklearn.svm import SVC  # noqa
from sklearn.model_selection import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa
from sklearn.pipeline import Pipeline  # noqa
from sklearn.model_selection import cross_val_score  # noqa

# https://mne.tools/0.16/auto_examples/decoding/plot_decoding_csp_space.html
n_components = 4  # pick some components(default=4)
svc = SVC(C=10, kernel='poly')
csp = CSP(n_components=n_components, reg='ledoit_wolf', norm_trace=False)
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
scores = []
#epochs_data = X_data
epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)

for train_idx, test_idx in cv.split(labels):
    y_train, y_test = labels[train_idx], labels[test_idx]

    X_train = csp.fit_transform(epochs_data[train_idx], y_train)
    X_test = csp.transform(epochs_data[test_idx])

    # fit classifier
    svc.fit(X_train, y_train)

    scores.append(svc.score(X_test, y_test))


# Printing the results
class_balance = np.mean(labels == labels[0])
class_balance = max(class_balance, 1. - class_balance)
print("Classification accuracy: %f / Chance level: %f"%(np.mean(scores),class_balance))
# Classification accuracy: 0.241637 / Chance level: 0.750000

"""# LFCNN

### Classify Paradigm: Event = {0,1,2} -> Pron-Inner-Vis
"""

import numpy as np
import matplotlib.pyplot as plt

import mne
from mne import io
from Data_extractions import Extract_block_data_from_subject, Extract_data_from_subject
from Data_processing import Filter_by_condition, Filter_by_class
from Utilitys import Ensure_dir, picks_from_channels

root_dir = '/content/drive/MyDrive/CPSC554X_Project/dataset'
N_S_list = [1,2,3,4,5,6,7,8]

N_B_arr=[1,2,3]
S = 0
epochs = []
data=dict()
y=dict()

# Data Parameters
datatype="EEG"
#Condition_list=["Pron","In", "Vis"]
Condition_list = ["all"]
#Classes_list=[ "up", "down", "left", "right"]
Classes_list = [ "all"]
#channels = ["CL", "CR", "OPZ", "OL", "OR"] # paradigm
channels = ["OZ", "OL", "CR", "CL", "FZ", "FR"] #class
picks = []

# Get picks for the selected channels
for i in channels:
  picks.append(picks_from_channels(i))

picks = np.concatenate(picks)
picks.tolist()

# In[]: Load Data
N_B = 1
N_S = 1

# Load a single subject to use the Epoched Object structure
X_S , Y = Extract_block_data_from_subject(root_dir,N_S,datatype,N_B=N_B)

Adquisition_eq="biosemi128"
montage = mne.channels.make_standard_montage(Adquisition_eq)
X_S.set_montage(montage)

# Get picks for the selected channels
count=1

for Classes in Classes_list: 
  for Cond in Condition_list:

    for N_S in N_S_list:

        # Load full subject's data
        X1, Y1 = Extract_data_from_subject(root_dir, N_S, datatype)

        # Filter by condition
        X_cond , Y_cond = Filter_by_condition(X1,Y1,Condition=Cond)

        # Filter by class
        X_class , Y_class = Filter_by_class(X_cond,Y_cond,Class=Classes)
        
        if count==1:  
            X_data = X_class
            Y_data = Y_class
        #    X_data = X1
        #    Y_data = Y1
            count=2
        else:
            #X_data = np.vstack((X_data,X1))
            #Y_data = np.vstack((Y_data,Y1))
            X_data = np.vstack((X_data,X_class))
            Y_data = np.vstack((Y_data,Y_class))

# Put all data
X_S._data=X_data
X_S.events=Y_data
X_S = X_S.pick_channels(picks)
X_S = X_S.crop(tmin = 0, tmax = 4)
X_data = X_S._data
labels = X_S.events[:, 2]

print(labels)

# https://github.com/zubara/mneflow/blob/master/examples/mneflow_example_tf2.ipynb
import numpy as np

import mne
mne.set_log_level(verbose='CRITICAL')
from mne.datasets import multimodal

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import tensorflow as tf
tf.get_logger().setLevel('ERROR')
tf.autograph.set_verbosity(0)

import mneflow

#Specify import options
import_opt = dict(savepath='../tfr/',  # path where TFR files will be saved
                  out_name='mne_sample_epochs',  # name of TFRecords files
                  fs=256,
                  input_type='trials',
                  target_type='int',
                  #picks={'eeg':'grad'},
                  scale=True,  # apply baseline_scaling
                  crop_baseline=True,  # remove baseline interval after scaling
                  decimate=None,
                  scale_interval=(0, 60),  # indices in time axis corresponding to baseline interval
                  n_folds=5,  # validation set size set to 20% of all data
                  overwrite=True,
                  segment=False,
                  test_set='holdout')

X = X_S.get_data()
#X = psd_all
y = labels
meta = mneflow.produce_tfrecords((X,y),**import_opt)
dataset = mneflow.Dataset(meta, train_batch=100)

lf_params = dict(n_latent=32, #number of latent factors
                  filter_length=17, #convolutional filter length in time samples
                  nonlin = tf.nn.relu,
                  padding = 'SAME',
                  pooling = 5,#pooling factor
                  stride = 5, #stride parameter for pooling layer
                  pool_type='max',
                  model_path = import_opt['savepath'],
                  dropout = .5,
                  l1_scope = ["weights"],
                  l1=3e-3)

model = mneflow.models.LFCNN(dataset, lf_params)
model.build()

#train the model for 10 epochs
model.train(n_epochs=25, eval_step=100, early_stopping=5)
model.plot_hist()

test_loss, test_acc = model.evaluate(meta['test_paths'])
# Updating log: test loss: 1.1427 test metric: 0.6135

# https://github.com/zubara/mneflow/blob/master/examples/mneflow_example_tf2.ipynb
import numpy as np

import mne
mne.set_log_level(verbose='CRITICAL')
from mne.datasets import multimodal

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import tensorflow as tf
tf.get_logger().setLevel('ERROR')
tf.autograph.set_verbosity(0)

import mneflow

#Specify import options
import_opt = dict(savepath='../tfr/',  # path where TFR files will be saved
                  out_name='mne_sample_epochs',  # name of TFRecords files
                  fs=256,
                  input_type='trials',
                  target_type='int',
                  #picks={'eeg':'grad'},
                  scale=True,  # apply baseline_scaling
                  crop_baseline=True,  # remove baseline interval after scaling
                  decimate=None,
                  scale_interval=(0, 60),  # indices in time axis corresponding to baseline interval
                  n_folds=5,  # validation set size set to 20% of all data
                  overwrite=True,
                  segment=False,
                  test_set='holdout')

#X = X_S.get_data()
#X = psd_all
X = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)
y = labels
meta = mneflow.produce_tfrecords((X,y),**import_opt)
dataset = mneflow.Dataset(meta, train_batch=100)

lf_params = dict(n_latent=32, #number of latent factors
                  filter_length=17, #convolutional filter length in time samples
                  nonlin = tf.nn.relu,
                  padding = 'SAME',
                  pooling = 5,#pooling factor
                  stride = 5, #stride parameter for pooling layer
                  pool_type='max',
                  model_path = import_opt['savepath'],
                  dropout = .5,
                  l1_scope = ["weights"],
                  l1=3e-3)

model = mneflow.models.LFCNN(dataset, lf_params)
model.build()

#train the model for 10 epochs
model.train(n_epochs=25, eval_step=100, early_stopping=5)
model.plot_hist()

test_loss, test_acc = model.evaluate(meta['test_paths'])

"""### Classify Word: Event = {31,32,33,34} -> Up-Down-Right-Left"""

import numpy as np
import matplotlib.pyplot as plt

import mne
from mne import io
from Data_extractions import Extract_block_data_from_subject, Extract_data_from_subject
from Data_processing import Filter_by_condition, Filter_by_class
from Utilitys import Ensure_dir, picks_from_channels

root_dir = '/content/drive/MyDrive/CPSC554X_Project/dataset'
N_S_list = [1,2,3,4,5,6,7,8]
N_B_arr=[1,2,3]
S = 0
epochs = []
data=dict()
y=dict()

# Data Parameters
datatype="EEG"
#Condition_list=["Pron","In", "Vis"]
Condition_list = ["Inner"]
#Classes_list=[ "up", "down", "left", "right"]
Classes_list = [ "all"]
channels = ["CL", "CR", "OPZ", "OL", "OR"] # paradigm
#channels = ["OZ", "OL", "CR", "CL", "FZ", "FR"] #class
picks = []

# Get picks for the selected channels
for i in channels:
  picks.append(picks_from_channels(i))

picks = np.concatenate(picks)
picks.tolist()

# In[]: Load Data
N_B = 1
N_S = 1

# Load a single subject to use the Epoched Object structure
X_S , Y = Extract_block_data_from_subject(root_dir,N_S,datatype,N_B=N_B)

Adquisition_eq="biosemi128"
montage = mne.channels.make_standard_montage(Adquisition_eq)
X_S.set_montage(montage)

# Get picks for the selected channels
count=1

for Classes in Classes_list: 
  for Cond in Condition_list:

    for N_S in N_S_list:

        # Load full subject's data
        X1, Y1 = Extract_data_from_subject(root_dir, N_S, datatype)

        # Filter by condition
        X_cond , Y_cond = Filter_by_condition(X1,Y1,Condition=Cond)
        Y_cond[:,1] = Y_cond[:,1] + 31
        Y_cond[:, [1, 2]] = Y_cond[:, [2, 1]]
        
        # Filter by class
        X_class , Y_class = Filter_by_class(X_cond,Y_cond,Class=Classes)
        
        if count==1:  
            X_data = X_class
            Y_data = Y_class
        #    X_data = X1
        #    Y_data = Y1
            count=2
        else:
            #X_data = np.vstack((X_data,X1))
            #Y_data = np.vstack((Y_data,Y1))
            X_data = np.vstack((X_data,X_class))
            Y_data = np.vstack((Y_data,Y_class))

# Put all data
X_S._data=X_data
X_S.events=Y_data
X_S = X_S.pick_channels(picks)
X_S = X_S.crop(tmin = 0, tmax = 4)
X_data = X_S._data
labels = X_S.events[:, 2]

# https://github.com/zubara/mneflow/blob/master/examples/mneflow_example_tf2.ipynb
import numpy as np

import mne
mne.set_log_level(verbose='CRITICAL')
from mne.datasets import multimodal

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import tensorflow as tf
tf.get_logger().setLevel('ERROR')
tf.autograph.set_verbosity(0)

import mneflow

#Specify import options
import_opt = dict(savepath='../tfr/',  # path where TFR files will be saved
                  out_name='mne_sample_epochs',  # name of TFRecords files
                  fs=256,
                  input_type='trials',
                  target_type='int',
                  #picks={'eeg':'grad'},
                  scale=True,  # apply baseline_scaling
                  crop_baseline=True,  # remove baseline interval after scaling
                  decimate=None,
                  scale_interval=(0, 60),  # indices in time axis corresponding to baseline interval
                  n_folds=5,  # validation set size set to 20% of all data
                  overwrite=True,
                  segment=False,
                  test_set='holdout')

X = X_S.get_data()
#X = psd_all
y = labels
meta = mneflow.produce_tfrecords((X,y),**import_opt)
dataset = mneflow.Dataset(meta, train_batch=100)

lf_params = dict(n_latent=32, #number of latent factors
                  filter_length=17, #convolutional filter length in time samples
                  nonlin = tf.nn.relu,
                  padding = 'SAME',
                  pooling = 5,#pooling factor
                  stride = 5, #stride parameter for pooling layer
                  pool_type='max',
                  model_path = import_opt['savepath'],
                  dropout = .5,
                  l1_scope = ["weights"],
                  l1=3e-3)

model = mneflow.models.LFCNN(dataset, lf_params)
model.build()

#train the model for 10 epochs
model.train(n_epochs=25, eval_step=100, early_stopping=5)
model.plot_hist()

test_loss, test_acc = model.evaluate(meta['test_paths'])
# test loss: 2.4804 test metric: 0.2877

# https://github.com/zubara/mneflow/blob/master/examples/mneflow_example_tf2.ipynb
import numpy as np

import mne
mne.set_log_level(verbose='CRITICAL')
from mne.datasets import multimodal

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import tensorflow as tf
tf.get_logger().setLevel('ERROR')
tf.autograph.set_verbosity(0)

import mneflow

#Specify import options
import_opt = dict(savepath='../tfr/',  # path where TFR files will be saved
                  out_name='mne_sample_epochs',  # name of TFRecords files
                  fs=256,
                  input_type='trials',
                  target_type='int',
                  #picks={'eeg':'grad'},
                  scale=True,  # apply baseline_scaling
                  crop_baseline=True,  # remove baseline interval after scaling
                  decimate=None,
                  scale_interval=(0, 60),  # indices in time axis corresponding to baseline interval
                  n_folds=5,  # validation set size set to 20% of all data
                  overwrite=True,
                  segment=False,
                  test_set='holdout')

#X = X_S.get_data()
#X = psd_all
X = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)
y = labels
meta = mneflow.produce_tfrecords((X,y),**import_opt)
dataset = mneflow.Dataset(meta, train_batch=100)

lf_params = dict(n_latent=32, #number of latent factors
                  filter_length=17, #convolutional filter length in time samples
                  nonlin = tf.nn.relu,
                  padding = 'SAME',
                  pooling = 5,#pooling factor
                  stride = 5, #stride parameter for pooling layer
                  pool_type='max',
                  model_path = import_opt['savepath'],
                  dropout = .5,
                  l1_scope = ["weights"],
                  l1=3e-3)

model = mneflow.models.LFCNN(dataset, lf_params)
model.build()

#train the model for 10 epochs
model.train(n_epochs=25, eval_step=100, early_stopping=5)
model.plot_hist()

test_loss, test_acc = model.evaluate(meta['test_paths'])
# test loss: 1.3911 test metric: 0.2530

"""# KNN with Riemannian

### Classify Paradigm: Event = {0,1,2} -> Pron-Inner-Vis
"""

import numpy as np
import matplotlib.pyplot as plt

import mne
from mne import io
from Data_extractions import Extract_block_data_from_subject, Extract_data_from_subject
from Data_processing import Filter_by_condition, Filter_by_class
from Utilitys import Ensure_dir, picks_from_channels

root_dir = '/content/drive/MyDrive/CPSC554X_Project/dataset'
N_S_list = [1,2,3,4,5,6,7,8]

N_B_arr=[1,2,3]
S = 0
epochs = []
data=dict()
y=dict()

# Data Parameters
datatype="EEG"
#Condition_list=["Pron","In", "Vis"]
Condition_list = ["all"]
#Classes_list=[ "up", "down", "left", "right"]
Classes_list = [ "all"]
#channels = ["CL", "CR", "OPZ", "OL", "OR"] # paradigm
channels = ["OZ", "OL", "CR", "CL", "FZ", "FR"] #class
picks = []

# Get picks for the selected channels
for i in channels:
  picks.append(picks_from_channels(i))

picks = np.concatenate(picks)
picks.tolist()

# In[]: Load Data
N_B = 1
N_S = 1

# Load a single subject to use the Epoched Object structure
X_S , Y = Extract_block_data_from_subject(root_dir,N_S,datatype,N_B=N_B)

Adquisition_eq="biosemi128"
montage = mne.channels.make_standard_montage(Adquisition_eq)
X_S.set_montage(montage)

# Get picks for the selected channels
count=1

for Classes in Classes_list: 
  for Cond in Condition_list:

    for N_S in N_S_list:

        # Load full subject's data
        X1, Y1 = Extract_data_from_subject(root_dir, N_S, datatype)

        # Filter by condition
        X_cond , Y_cond = Filter_by_condition(X1,Y1,Condition=Cond)

        # Filter by class
        X_class , Y_class = Filter_by_class(X_cond,Y_cond,Class=Classes)
        
        if count==1:  
            X_data = X_class
            Y_data = Y_class
        #    X_data = X1
        #    Y_data = Y1
            count=2
        else:
            #X_data = np.vstack((X_data,X1))
            #Y_data = np.vstack((Y_data,Y1))
            X_data = np.vstack((X_data,X_class))
            Y_data = np.vstack((Y_data,Y_class))

# Put all data
X_S._data=X_data
X_S.events=Y_data
X_S = X_S.pick_channels(picks)
X_S = X_S.crop(tmin = 0, tmax = 4)
X_data = X_S._data
labels = X_S.events[:, 2]

import pyriemann
from sklearn.model_selection import cross_val_score

from pyriemann.estimation import Covariances
from pyriemann.tangentspace import TangentSpace

from sklearn.pipeline import make_pipeline
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multioutput import MultiOutputClassifier
from pyriemann.estimation import XdawnCovariances
from pyriemann.classification import MDM
from pyriemann.classification import KNearestNeighbor
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)
from sklearn.preprocessing import FunctionTransformer

n_components = 'cov'  # pick some components

# Define a monte-carlo cross-validation generator (reduce variance):
cv = KFold(n_splits=10, shuffle=True, random_state=42)
#cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
pr = np.zeros(len(labels))
epochs_data = X_S._data


print("Multiclass classification with Covariance + KNN")
# Perform a simple covariance matrix estimation for each given trial.

clf = make_pipeline(Covariances(n_components),KNearestNeighbor())
#print(Covariances(n_components))
#knn = KNearestNeighbor()
for train_idx, test_idx in cv.split(epochs_data):
    y_train, y_test = labels[train_idx], labels[test_idx]
    clf.fit(epochs_data[train_idx], y_train)
    #print(epochs_data[train_idx].shape)
    #print(epochs_data[train_idx].shape)
    #print(pr[test_idx].shape)
    print('1')
    pr[test_idx] = clf.predict(epochs_data[test_idx])

print(classification_report(labels, pr))

import pyriemann
from sklearn.model_selection import cross_val_score

from pyriemann.estimation import Covariances
from pyriemann.tangentspace import TangentSpace

from sklearn.pipeline import make_pipeline
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multioutput import MultiOutputClassifier
from pyriemann.estimation import XdawnCovariances
from pyriemann.classification import MDM
from pyriemann.classification import KNearestNeighbor
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)



n_components = 'cov'  # pick some components

# Define a monte-carlo cross-validation generator (reduce variance):
cv = KFold(n_splits=10, shuffle=True, random_state=42)
#[psd_all, freqs] = mne.time_frequency.psd_welch(X_S)
#epochs_data = psd_all
#epochs_data = X_data
epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)
pr = np.zeros(len(labels))
print("Multiclass classification with Covariance + KNN")
# Perform a simple covariance matrix estimation for each given trial.

clf = make_pipeline(Covariances(), KNearestNeighbor())
#clf = KNearestNeighbor()

for train_idx, test_idx in cv.split(epochs_data):
    y_train, y_test = labels[train_idx], labels[test_idx]
    print("1")
    clf.fit(epochs_data[train_idx], y_train)
    pr[test_idx] = clf.predict(epochs_data[test_idx])

print(classification_report(labels, pr))

"""### Classify Word: Event = {31,32,33,34} -> Up-Down-Right-Left"""

import numpy as np
import matplotlib.pyplot as plt

import mne
from mne import io
from Data_extractions import Extract_block_data_from_subject, Extract_data_from_subject
from Data_processing import Filter_by_condition, Filter_by_class
from Utilitys import Ensure_dir, picks_from_channels

root_dir = '/content/drive/MyDrive/CPSC554X_Project/dataset'
N_S_list = [1,2,3,4,5,6,7,8]
N_B_arr=[1,2,3]
S = 0
epochs = []
data=dict()
y=dict()

# Data Parameters
datatype="EEG"
#Condition_list=["Pron","In", "Vis"]
Condition_list = ["Inner"]
#Classes_list=[ "up", "down", "left", "right"]
Classes_list = [ "all"]
channels = ["CL", "CR", "OPZ", "OL", "OR"] # paradigm
#channels = ["OZ", "OL", "CR", "CL", "FZ", "FR"] #class
picks = []

# Get picks for the selected channels
for i in channels:
  picks.append(picks_from_channels(i))

picks = np.concatenate(picks)
picks.tolist()

# In[]: Load Data
N_B = 1
N_S = 1

# Load a single subject to use the Epoched Object structure
X_S , Y = Extract_block_data_from_subject(root_dir,N_S,datatype,N_B=N_B)

Adquisition_eq="biosemi128"
montage = mne.channels.make_standard_montage(Adquisition_eq)
X_S.set_montage(montage)

# Get picks for the selected channels
count=1

for Classes in Classes_list: 
  for Cond in Condition_list:

    for N_S in N_S_list:

        # Load full subject's data
        X1, Y1 = Extract_data_from_subject(root_dir, N_S, datatype)

        # Filter by condition
        X_cond , Y_cond = Filter_by_condition(X1,Y1,Condition=Cond)
        Y_cond[:,1] = Y_cond[:,1] + 31
        Y_cond[:, [1, 2]] = Y_cond[:, [2, 1]]
        
        # Filter by class
        X_class , Y_class = Filter_by_class(X_cond,Y_cond,Class=Classes)
        
        if count==1:  
            X_data = X_class
            Y_data = Y_class
        #    X_data = X1
        #    Y_data = Y1
            count=2
        else:
            #X_data = np.vstack((X_data,X1))
            #Y_data = np.vstack((Y_data,Y1))
            X_data = np.vstack((X_data,X_class))
            Y_data = np.vstack((Y_data,Y_class))

# Put all data
X_S._data=X_data
X_S.events=Y_data
X_S = X_S.pick_channels(picks)
X_S = X_S.crop(tmin = 0, tmax = 4)
X_data = X_S._data
labels = X_S.events[:, 2]

import pyriemann
from sklearn.model_selection import cross_val_score

from pyriemann.estimation import Covariances
from pyriemann.tangentspace import TangentSpace

from sklearn.pipeline import make_pipeline
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multioutput import MultiOutputClassifier
from pyriemann.estimation import XdawnCovariances
from pyriemann.classification import MDM
from pyriemann.classification import KNearestNeighbor
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)



n_components = 'cov'  # pick some components

# Define a monte-carlo cross-validation generator (reduce variance):
cv = KFold(n_splits=10, shuffle=True, random_state=42)
#[psd_all, freqs] = mne.time_frequency.psd_welch(X_S)
#epochs_data = psd_all
epochs_data = X_data

pr = np.zeros(len(labels))
print("Multiclass classification with Covariance + KNN")
# Perform a simple covariance matrix estimation for each given trial.

clf = make_pipeline(Covariances(), KNearestNeighbor())
#clf = KNearestNeighbor()

for train_idx, test_idx in cv.split(epochs_data):
    y_train, y_test = labels[train_idx], labels[test_idx]
    print("1")
    clf.fit(epochs_data[train_idx], y_train)
    pr[test_idx] = clf.predict(epochs_data[test_idx])

print(classification_report(labels, pr))

import pyriemann
from sklearn.model_selection import cross_val_score

from pyriemann.estimation import Covariances
from pyriemann.tangentspace import TangentSpace

from sklearn.pipeline import make_pipeline
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multioutput import MultiOutputClassifier
from pyriemann.estimation import XdawnCovariances
from pyriemann.classification import MDM
from pyriemann.classification import KNearestNeighbor
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)



n_components = 'cov'  # pick some components

# Define a monte-carlo cross-validation generator (reduce variance):
cv = KFold(n_splits=10, shuffle=True, random_state=42)
#[psd_all, freqs] = mne.time_frequency.psd_welch(X_S)
#epochs_data = psd_all
#epochs_data = X_data
epochs_data = np.concatenate((X_data[:, :, 0:250], X_data[:, :, 250:500],X_data[:, :, 500:750],X_data[:, :, 750:1000]),axis = 0)
labels = np.concatenate((X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2],X_S.events[:, 2]), axis = 0)
pr = np.zeros(len(labels))
print("Multiclass classification with Covariance + KNN")
# Perform a simple covariance matrix estimation for each given trial.

clf = make_pipeline(Covariances(), KNearestNeighbor())
#clf = KNearestNeighbor()

for train_idx, test_idx in cv.split(epochs_data):
    y_train, y_test = labels[train_idx], labels[test_idx]
    print("1")
    clf.fit(epochs_data[train_idx], y_train)
    pr[test_idx] = clf.predict(epochs_data[test_idx])

print(classification_report(labels, pr))

import pyriemann
from sklearn.model_selection import cross_val_score

from pyriemann.estimation import Covariances
from pyriemann.tangentspace import TangentSpace

from sklearn.pipeline import make_pipeline
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multioutput import MultiOutputClassifier
from pyriemann.estimation import XdawnCovariances
from pyriemann.classification import MDM
from pyriemann.classification import KNearestNeighbor
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)

n_components = 'cov'  # pick some components

# Define a monte-carlo cross-validation generator (reduce variance):
cv = KFold(n_splits=10, shuffle=True, random_state=42)
[psd_all, freqs] = mne.time_frequency.psd_welch(X_S)
epochs_data = psd_all
#epochs_data = X_data

pr = np.zeros(len(labels))
print("Multiclass classification with Covariance + KNN")
# Perform a simple covariance matrix estimation for each given trial.

clf = make_pipeline(Covariances(), KNearestNeighbor())
#clf = KNearestNeighbor()

for train_idx, test_idx in cv.split(epochs_data):
    y_train, y_test = labels[train_idx], labels[test_idx]
    print("1")
    clf.fit(epochs_data[train_idx], y_train)
    pr[test_idx] = clf.predict(epochs_data[test_idx])

print(classification_report(labels, pr))